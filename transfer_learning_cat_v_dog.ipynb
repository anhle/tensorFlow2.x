{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_learning_cat-v-dog.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhle/tensorFlow2.x/blob/master/transfer_learning_cat_v_dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZWP7Jck_RBO",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning Using Pretrained ConvNets\n",
        "\n",
        "we will discuss how to classify cats vs dogs images by using transfer learning from a pre-trained network. This will allows us to get higher accuracies than we saw by training our network from scratch.\n",
        "\n",
        "A **pre-trained model ** is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. We can either use the pretrained model as it is or transfer learning using the pretrained convents. The intuition behind **transfer learning **is that if this model trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. We can leverage these learned feature maps without having to train a large model on a large dataset by using these models as the basis of our own model specific to our task. There are 2 scenarios of transfer learning using a pretrained model:\n",
        "\n",
        "1. **Feature Extraction **- use the representations of learned by a previous network to extract meaningful features from new samples. We simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that we can repurpose the feature maps learned previously for our dataset. Do we use the entire pretrained model or just the convolutional base? - We use the feature extraction portion of these pretrained convnets (convolutional base) since they are likely to be generic features and learned concepts over a picture. However, the classification part of the pretrained model is often specific to original classification task, and subsequently specific to the set of classes on which the model was trained.\n",
        "2. **Fine-Tuning**  - unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added classifier layers as well as the last layers of the frozen model. This allows us to \"fine tune\" the higher order feature representations in addition to our final classifier in order to make them more relevant for the specific task involved.\n",
        "\n",
        "We will follow the general machine learning workflow: \n",
        "1. Examine and understand data \n",
        "2. Build an input pipeline - using Keras ImageDataGenerator as we did in the image classification tutorial \n",
        "3. Compose our model * Load in our pretrained model (and pretrained weights) * Stack our classification layers on top \n",
        "4. Train our model\n",
        "5. Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lko_GbY-71J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Import TensorFlow\n",
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K1ikHb7Ahvo",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n",
        "* Download data - cats_and_dogs_filtered.zip\n",
        "We will download a filtered version of Kaggle's Dogs vs Cats dataset. Then store the downloaded zip file to the \"/tmp/\" directory.\n",
        "\n",
        "* Create Image Data Generator with Image Augmentation: we will use ImageDataGenerator to rescale the images.\n",
        "\n",
        "* Create the train generator and specify where the train dataset directory, image size, batch size and it's a binary classification.\n",
        "\n",
        "* Create the validation generator with similar approach as the train generator with the flow_from_directory() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPALmAVNBVvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "       -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "local_zip = '//tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'train')\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats') # Directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs') # Directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats') # Directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')# Directory with our validation dog pictures\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "\n",
        "image_size = 150 # All images will be resized to 160x160\n",
        "batch_size = 32\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = batch_size,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (image_size, image_size))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = batch_size,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (image_size, image_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBJhq640Djap",
        "colab_type": "text"
      },
      "source": [
        "## Create the base model from the pre-trained\n",
        "\n",
        "First, we need to pick which intermediate layer of InceptionV3 (MobileNet V2) we will use for feature extraction. A common practice is to use the output of the very last layer before the flatten operation, the so-called \"bottleneck layer\". The reasoning here is that the following fully-connected layers will be too specialized to the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
        "\n",
        "Let's instantiate an InceptionV3 (MobileNet V2) model pre-loaded with weights trained on ImageNet. By specifying the*** include_top=False ***argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.\n",
        "\n",
        "Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "Freeze the convolutional base\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkLkkRhDyq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (image_size, image_size, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwdD4gLkFSue",
        "colab_type": "text"
      },
      "source": [
        "## Add a classification head\n",
        "Now let's add a few layers on top of the base model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpTkEuxBFaPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1, activation='sigmoid')(x)           \n",
        "\n",
        "model = Model( pre_trained_model.input, x) \n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveMI-giF5HD",
        "colab_type": "text"
      },
      "source": [
        "# Train the model\n",
        "After training for 10 epochs, we are able to get ~94% accuracy.\n",
        "\n",
        "If you have more time, train it to convergence (50 epochs, ~96% accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WcskWe1F-fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(\n",
        "            train_generator,\n",
        "            validation_data = validation_generator,\n",
        "            steps_per_epoch = 100,\n",
        "            epochs = 20,\n",
        "            validation_steps = 50,\n",
        "            verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZGdWScHWJO",
        "colab_type": "text"
      },
      "source": [
        "## Learning curves\n",
        "Let's take a look at the learning curves of the training and validation accuracy / loss, when using the MobileNet V2 base model as a fixed feature extractor.\n",
        "\n",
        "If you train to convergence (epochs=50) the resulting graph should look like this:\n",
        "\n",
        "Before fine tuning, the model reaches 96% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aWE4KvGGTB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mXPg-pxIfEr",
        "colab_type": "text"
      },
      "source": [
        "## Fine tuning\n",
        "In our feature extraction experiment, we were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were not updated during training. One way to increase performance even further is to \"fine-tune\" the weights of the top layers of the pre-trained model alongside the training of the top-level classifier. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.\n",
        "\n",
        "Additionally, the reasoning behind fine-tuning the top layers of the pre-trained model rather than all layers of the pre-trained model is the following: in a convnet, the higher up a layer is, the more specialized it is. The first few layers in a convnet learned very simple and generic features, which generalize to almost all types of images. But as you go higher up, the features are increasingly more specific to the dataset that the model was trained on. The goal of fine-tuning is to adapt these specialized features to work with the new dataset.\n",
        "\n",
        "Un-freeze the top layers of the model\n",
        "All we need to do is unfreeze the base_model, and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training."
      ]
    }
  ]
}