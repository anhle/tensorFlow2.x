{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chexpert.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhle/tensorFlow2.x/blob/master/Chexpert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sx5iHPxW8n9",
        "colab_type": "text"
      },
      "source": [
        "# Attempting to replicate the results from:\n",
        "CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison\n",
        "https://arxiv.org/abs/1901.07031\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6czcIVsWNKC-",
        "colab_type": "text"
      },
      "source": [
        "# Download CheXpert Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja4TktpBNvCt",
        "colab_type": "code",
        "outputId": "1d8da436-6ac1-42ee-97be-77d70b4a49ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeb3e33pNQ28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import wget\n",
        "import os\n",
        "\n",
        "dir_base = '/content' \n",
        "chexpert_url = 'http://download.cs.stanford.edu/deep/CheXpert-v1.0-small.zip' \n",
        "        \n",
        "zipname = wget.download(chexpert_url,dir_base)\n",
        "\n",
        "zip_file = zipfile.ZipFile(os.path.join(dir_base,zipname))\n",
        "zip_file.extractall(dir_base)\n",
        "zip_file.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMAt8sFnZLR_",
        "colab_type": "code",
        "outputId": "5662d7b6-81de-46c3-dc85-4ba64590b467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import datetime\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFna7pAbXM6w",
        "colab_type": "text"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "## Uncertainty Approaches\n",
        "The CheXpert paper outlines several different approaches to mapping using the uncertainty labels in the data:\n",
        "\n",
        "* Ignoring - essentially removing from the calculation in the loss function\n",
        "* Binary mapping - sending uncertain values to either 0 or 1\n",
        "* Prevalence mapping - use the rate of prevelance of the feature as it's target value\n",
        "* Self-training - consider the uncertain values as unlabeled\n",
        "* 3-Class Classification - retain a separate value for uncertain and try to predict it as a class in its own right\n",
        "\n",
        "The paper gives the results of different experiments with the above approaches and indicates the most accurate approach for each feature.\n",
        "\n",
        "Approach/Feature\tAtelectasis\tCardiomegaly\tConsolidation\tEdema\tPleuralEffusion\n",
        "U-Ignore\t0.818(0.759,0.877)\t0.828(0.769,0.888)\t0.938(0.905,0.970)\t0.934(0.893,0.975)\t0.928(0.894,0.962)\n",
        "U-Zeros\t0.811(0.751,0.872)\t0.840(0.783,0.897)\t0.932(0.898,0.966)\t0.929(0.888,0.970)\t0.931(0.897,0.965)\n",
        "U-Ones\t0.858(0.806,0.910)\t0.832(0.773,0.890)\t0.899(0.854,0.944)\t0.941(0.903,0.980)\t0.934(0.901,0.967)\n",
        "U-Mean\t0.821(0.762,0.879)\t0.832(0.771,0.892)\t0.937(0.905,0.969)\t0.939(0.902,0.975)\t0.930(0.896,0.965)\n",
        "U-SelfTrained\t0.833(0.776,0.890)\t0.831(0.770,0.891)\t0.939(0.908,0.971)\t0.935(0.896,0.974)\t0.932(0.899,0.966)\n",
        "U-MultiClass\t0.821(0.763,0.879)\t0.854(0.800,0.909)\t0.937(0.905,0.969)\t0.928(0.887,0.968)\t0.936(0.904,0.967)\n",
        "\n",
        "The binary mapping approaches (U-Ones and U-Zeros) are easiest to implement and so to begin with we take the best option between U-Ones and U-Zeros for each feature\n",
        "\n",
        "* Atelectasis U-Ones\n",
        "* Cardiomegaly U-Zeros\n",
        "* Consolidation U-Zeros\n",
        "* Edema U-Ones\n",
        "* Pleural Effusion U-Zeros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxN0XZCAWqRL",
        "colab_type": "code",
        "outputId": "aee5069d-b3f7-4c0d-cc45-906c62d072ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Path to images and category labels in data dir\n",
        "_DATA_DIR = \"/content/CheXpert-v1.0-small\"\n",
        "_IMAGE_DIR = './'\n",
        "train_file = os.path.join(_DATA_DIR,'train.csv')\n",
        "valid_file = os.path.join(_DATA_DIR,'valid.csv')\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "valid_df = pd.read_csv(valid_file)\n",
        "\n",
        "#Our classfication problem consists of only 5 classes as mentioned in the CheXpert Competition.\n",
        "class_names = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
        "\n",
        "train_df = train_df[train_df['Frontal/Lateral']=='Frontal']\n",
        "train_path, y = train_df[\"Path\"].as_matrix(), train_df[class_names]\n",
        "\n",
        "train_counts = train_df.shape[0]\n",
        "valid_counts = valid_df.shape[0]\n",
        "print('train counts', train_counts)\n",
        "print('valid counts', valid_counts)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train counts 191027\n",
            "valid counts 234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EE7PDc5LsCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4oAX0kXO0Cs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.head(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIm3jHLIHqug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sample_counts(file, class_names):\n",
        "    \"\"\"\n",
        "    Get total and class-wise positive sample count of a dataset\n",
        "    Arguments:\n",
        "    output_dir - str, folder of dataset.csv\n",
        "    dataset - str, train|dev|test\n",
        "    class_names - list of str, target classes\n",
        "    Returns:\n",
        "    total_count - int\n",
        "    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file)\n",
        "    total_count = df.shape[0]\n",
        "    labels = df[class_names].as_matrix()\n",
        "    positive_counts = np.sum(labels, axis=0)\n",
        "    #class_positive_counts = dict(zip(class_names, positive_counts))\n",
        "    class_positive_counts = {key:value for key, value in zip(class_names, positive_counts)}\n",
        "    return total_count, class_positive_counts\n",
        "\n",
        "def get_class_weights(total_counts, class_positive_counts, multiply):\n",
        "    \"\"\"\n",
        "    Calculate class_weight used in training\n",
        "    Arguments:\n",
        "    total_counts - int\n",
        "    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n",
        "    multiply - int, positve weighting multiply\n",
        "    use_class_balancing - boolean\n",
        "    Returns:\n",
        "    class_weight - dict of dict, ex: {\"Effusion\": { 0: 0.01, 1: 0.99 }, ... }\n",
        "    \"\"\"\n",
        "    def get_single_class_weight(pos_counts, total_counts):\n",
        "        denominator = (total_counts - pos_counts) * multiply + pos_counts\n",
        "        return {\n",
        "            0: pos_counts / denominator,\n",
        "            1: (denominator - pos_counts) / denominator,\n",
        "        }\n",
        "\n",
        "    class_names = list(class_positive_counts.keys())\n",
        "    label_counts = np.array(list(class_positive_counts.values()))\n",
        "    class_weights = []\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_weights.append(get_single_class_weight(label_counts[i], total_counts))\n",
        "\n",
        "    return class_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBD-l1TJHwpj",
        "colab_type": "code",
        "outputId": "3ff47975-70ac-4c6c-b1b0-5bad465bc385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "positive_weights_multiply=1\n",
        "train_counts, train_pos_counts = get_sample_counts(train_file, class_names)\n",
        "valid_counts, _ = get_sample_counts(valid_file, class_names)\n",
        "class_weights = get_class_weights(\n",
        "    train_counts,\n",
        "    train_pos_counts,\n",
        "    multiply=positive_weights_multiply,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlhfzyE1ZEG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('train count: ',train_counts)\n",
        "print('valid count: ',valid_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o80noQaVON56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "\n",
        "class CheXpertDataGenerator(keras.utils.Sequence):\n",
        "    'Data Generetor for CheXpert'\n",
        "\n",
        "    def __init__(self, dataset_csv_file, class_names, source_image_dir, batch_size=16,\n",
        "                 target_size=(224, 224), policy = \"zeroes\", augmenter=None, verbose=0,\n",
        "                 shuffle_on_epoch_end=False, random_state=1):\n",
        "\n",
        "        self.dataset_df = pd.read_csv(dataset_csv_file)\n",
        "        self.source_image_dir = source_image_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.augmenter = augmenter\n",
        "        self.verbose = verbose\n",
        "        self.shuffle = shuffle_on_epoch_end\n",
        "        self.random_state = random_state\n",
        "        self.class_names = class_names\n",
        "        self.policy = policy\n",
        "        self.prepare_dataset()\n",
        "        self.steps = int(np.ceil(len(self.x_path) / float(self.batch_size)))\n",
        "\n",
        "        # print('steps..', self.steps)\n",
        "    def __bool__(self):\n",
        "        return True\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print('idx....', idx)\n",
        "        batch_x_path = self.x_path[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = np.asarray([self.load_image(x_path) for x_path in batch_x_path])\n",
        "        batch_x = self.transform_batch_images(batch_x)\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def load_image(self, image_file):\n",
        "        image_path = os.path.join(self.source_image_dir, image_file)\n",
        "        image = Image.open(image_path)\n",
        "        image_array = np.asarray(image.convert(\"RGB\"))\n",
        "        image_array = image_array / 255.\n",
        "        image_array = resize(image_array, self.target_size)\n",
        "        return image_array\n",
        "\n",
        "    def transform_batch_images(self, batch_x):\n",
        "        if self.augmenter is not None:\n",
        "            batch_x = self.augmenter.augment_images(batch_x)\n",
        "        imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
        "        imagenet_std = np.array([0.229, 0.224, 0.225])\n",
        "        if batch_x.shape == imagenet_mean.shape:\n",
        "            batch_x = (batch_x - imagenet_mean) / imagenet_std\n",
        "        return batch_x\n",
        "\n",
        "    def get_y_true(self):\n",
        "        if self.shuffle:\n",
        "            raise ValueError(\"\"\"\n",
        "            You're trying run get_y_true() when generator option 'shuffle_on_epoch_end' is True.\n",
        "            \"\"\")\n",
        "        return self.y[:self.steps*self.batch_size, :]\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        self.dataset_df = self.dataset_df[self.dataset_df['Frontal/Lateral'] == 'Frontal']\n",
        "        df = self.dataset_df.sample(frac=1., random_state=self.random_state)\n",
        "        df.fillna(0, inplace=True)\n",
        "        self.x_path, y_df = df[\"Path\"].as_matrix(), df[self.class_names]\n",
        "        self.class_ones = ['Atelectasis', 'Cardiomegaly']\n",
        "        self.y = np.empty(y_df.shape, dtype=int)\n",
        "        # print(y_ar.shape)\n",
        "        for i, (index, row) in enumerate(y_df.iterrows()):\n",
        "            labels = []\n",
        "            for cls in self.class_names:\n",
        "                #         print(cls)\n",
        "                curr_val = row[cls]\n",
        "                #         print(curr_val)\n",
        "                feat_val = 0\n",
        "                if curr_val:\n",
        "                    curr_val = float(curr_val)\n",
        "                    if curr_val == 1:\n",
        "                        feat_val = 1\n",
        "                    elif curr_val == -1:\n",
        "                        if self.policy == \"ones\":\n",
        "                            feat_val = 1\n",
        "                        elif self.policy == \"zeroes\":\n",
        "                            feat_val = 0\n",
        "                        elif self.policy == \"mixed\":\n",
        "                            if cls in self.class_ones:\n",
        "                                feat_val = 1\n",
        "                            else:\n",
        "                                feat_val = 0\n",
        "                        else:\n",
        "                            feat_val = 0\n",
        "                    else:\n",
        "                        feat_val = 0\n",
        "                else:\n",
        "                    feat_val = 0\n",
        "                labels.append(feat_val)\n",
        "            # print(labels)\n",
        "            self.y[i] = labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.random_state += 1\n",
        "            self.prepare_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueE_qfUfmmCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "\n",
        "augmenter = iaa.Sequential(\n",
        "    [\n",
        "        iaa.Fliplr(0.5),\n",
        "        iaa.Affine(scale=(0.5, 1.5)),\n",
        "        iaa.CropAndPad(percent=(-0.25, 0.25)),\n",
        "        iaa.Noop(),\n",
        "    ],\n",
        "    random_order=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_OL5KQgj9Ds",
        "colab_type": "code",
        "outputId": "974d8111-b647-475d-a540-b84e15f756fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "batch_size = 32\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "_IMAGE_DIR = './'\n",
        "\n",
        "train_data = CheXpertDataGenerator(dataset_csv_file=train_file,\n",
        "            class_names=class_names,\n",
        "            source_image_dir=_IMAGE_DIR,\n",
        "            batch_size=batch_size,\n",
        "            target_size=(HEIGHT, WIDTH),\n",
        "            augmenter=augmenter,\n",
        "            policy = 'mixed'\n",
        "    )\n",
        "\n",
        "valid_data = CheXpertDataGenerator(dataset_csv_file=valid_file,\n",
        "            class_names=class_names,\n",
        "            source_image_dir=_IMAGE_DIR,\n",
        "            batch_size=batch_size,\n",
        "            target_size=(HEIGHT, WIDTH),\n",
        "            augmenter=None,\n",
        "            policy ='mixed'\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9M7Hh_dnf-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#from tensorflow.keras import backend as k\n",
        "from keras.applications import DenseNet121\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "from tensorflow.python.keras import backend as k\n",
        "\n",
        "import importlib\n",
        "\n",
        "class ModelFactory:\n",
        "    \"\"\"\n",
        "    Model facotry for Keras default models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models_ = dict(\n",
        "            DenseNet121=dict(\n",
        "                input_shape=(224, 224, 3),\n",
        "                module_name=\"densenet\",\n",
        "                last_conv_layer=\"bn\",\n",
        "            ),\n",
        "            ResNet50=dict(\n",
        "                input_shape=(224, 224, 3),\n",
        "                module_name=\"resnet50\",\n",
        "                last_conv_layer=\"activation_49\",\n",
        "            ),\n",
        "            InceptionV3=dict(\n",
        "                input_shape=(299, 299, 3),\n",
        "                module_name=\"inception_v3\",\n",
        "                last_conv_layer=\"mixed10\",\n",
        "            ),\n",
        "            InceptionResNetV2=dict(\n",
        "                input_shape=(299, 299, 3),\n",
        "                module_name=\"inception_resnet_v2\",\n",
        "                last_conv_layer=\"conv_7b_ac\",\n",
        "            ),\n",
        "            Xception=dict(\n",
        "                input_shape=(224, 224, 3),\n",
        "                pooling=\"avg\"\n",
        "            ),\n",
        "            NASNetLarge=dict(\n",
        "                input_shape=(331, 331, 3),\n",
        "                module_name=\"nasnet\",\n",
        "                last_conv_layer=\"activation_260\",\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def get_last_conv_layer(self, model_name):\n",
        "        return self.models_[model_name][\"last_conv_layer\"]\n",
        "\n",
        "    def get_input_size(self, model_name):\n",
        "        return self.models_[model_name][\"input_shape\"][:2]\n",
        "\n",
        "    def get_model(self, class_names, model_name=\"DenseNet121\", use_base_weights=True,\n",
        "                  weights_path=None, input_shape=None):\n",
        "\n",
        "        if use_base_weights is True:\n",
        "            base_weights = \"imagenet\"\n",
        "        else:\n",
        "            base_weights = None\n",
        "\n",
        "        base_model_class = getattr(\n",
        "            importlib.import_module(\n",
        "                \"keras.applications.\" + self.models_[model_name]['module_name']\n",
        "            ),\n",
        "            model_name)\n",
        "\n",
        "        if input_shape is None:\n",
        "            input_shape = self.models_[model_name][\"input_shape\"]\n",
        "\n",
        "        img_input = Input(shape=input_shape)\n",
        "\n",
        "        base_model = base_model_class(\n",
        "            include_top=False,\n",
        "            input_tensor=img_input,\n",
        "            input_shape=input_shape,\n",
        "            weights=base_weights,\n",
        "            pooling=\"avg\")\n",
        "        x = base_model.output\n",
        "        predictions = Dense(len(class_names), activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "        model = Model(inputs=img_input, outputs=predictions)\n",
        "\n",
        "        if weights_path == \"\":\n",
        "            weights_path = None\n",
        "\n",
        "        if weights_path is not None:\n",
        "            print(\"load model weights_path\", weights_path)\n",
        "            model.load_weights(weights_path)\n",
        "        return model\n",
        "\n",
        "\n",
        "def DenseNet(height, width, channels, classes):\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(height, width, channels))\n",
        "    for layer in base_model.layers[-4:]:\n",
        "        layer.trainable = False\n",
        "    x = base_model.layers[-1].output  # es la salida del ultimo activation despues del add\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    #x = layers.GlobalMaxPool2D()(x)\n",
        "    # output layer\n",
        "    #---1) NO LINEAL + LINEAL\n",
        "    # prepredictions = Dense(256, activation='relu')(x)\n",
        "    # predictions = Dense(classes, activation='sigmoid')(prepredictions)\n",
        "\n",
        "    #---2) LINEAL\n",
        "    predictions = Dense(classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkZD4Y94nmW4",
        "colab_type": "code",
        "outputId": "cafd185d-8e0e-4d61-eb16-d0696f3b376b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_file_path = '/content/best_weights_1555982768.7076797.h5'\n",
        "model_type = \"DenseNet121\"\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model_factory = ModelFactory()\n",
        "\n",
        "model = model_factory.get_model(\n",
        "                        class_names,\n",
        "                        model_name=model_type,\n",
        "                        use_base_weights=True,\n",
        "                        weights_path=model_file_path,\n",
        "                        input_shape=(HEIGHT, WIDTH, 3)\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\", \"binary_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load model weights_path /content/best_weights_1555982768.7076797.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsBzIYkTAMVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import keras.backend as kb\n",
        "import json\n",
        "\n",
        "import shutil\n",
        "\n",
        "class MultipleClassAUROC(Callback):\n",
        "    \"\"\"\n",
        "    Monitor mean AUROC and update model\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence, class_names, weights_path, stats=None, workers=1):\n",
        "        super(Callback, self).__init__()\n",
        "        self.sequence = sequence\n",
        "        self.workers = workers\n",
        "        self.class_names = class_names\n",
        "        self.weights_path = weights_path\n",
        "        self.best_weights_path = os.path.join(\n",
        "            os.path.split(weights_path)[0],\"best_\"+os.path.split(weights_path)[1],\n",
        "        )\n",
        "        self.best_auroc_log_path = os.path.join(\n",
        "            os.path.split(weights_path)[0],\n",
        "            \"best_auroc.log\",\n",
        "        )\n",
        "        self.stats_output_path = os.path.join(\n",
        "            os.path.split(weights_path)[0],\n",
        "            \".training_stats.json\"\n",
        "        )\n",
        "        # for resuming previous training\n",
        "        if stats:\n",
        "            self.stats = stats\n",
        "        else:\n",
        "            self.stats = {\"best_mean_auroc\": 0}\n",
        "\n",
        "        # aurocs log\n",
        "        self.aurocs = {}\n",
        "        for c in self.class_names:\n",
        "            self.aurocs[c] = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \"\"\"\n",
        "        Calculate the average AUROC and save the best model weights according\n",
        "        to this metric.\n",
        "        \"\"\"\n",
        "        print(\"\\n*********************************\")\n",
        "        self.stats[\"lr\"] = float(kb.eval(self.model.optimizer.lr))\n",
        "        print(\"current learning rate:\", self.stats['lr'])\n",
        "\n",
        "        \"\"\"\n",
        "        y_hat shape: (#samples, len(class_names))\n",
        "        y: [(#samples, 1), (#samples, 1) ... (#samples, 1)]\n",
        "        \"\"\"\n",
        "        y_hat = self.model.predict_generator(self.sequence, workers=self.workers)\n",
        "        y = self.sequence.get_y_true()\n",
        "\n",
        "        # print(\"****************************\")\n",
        "        # print('y_hat', y_hat.shape)\n",
        "        # print(y_hat)\n",
        "        # print('y', y.shape)\n",
        "        # print(y)\n",
        "        # print(\"****************************\")\n",
        "\n",
        "        print(\"*** Epoch# %d dev auroc ***\" % (epoch + 1))\n",
        "        current_auroc = []\n",
        "        for i in range(len(self.class_names)):\n",
        "            try:\n",
        "                score = roc_auc_score(y[:, i], y_hat[:, i])\n",
        "            except ValueError:\n",
        "                score = 0\n",
        "            self.aurocs[self.class_names[i]].append(score)\n",
        "            current_auroc.append(score)\n",
        "            print(\"%d. %s: %f\" % ((i+1), self.class_names[i], score))\n",
        "        print(\"*********************************\")\n",
        "\n",
        "        # customize your multiple class metrics here\n",
        "        mean_auroc = np.mean(current_auroc)\n",
        "        print(\"mean auroc: %f\" % (mean_auroc))\n",
        "        if mean_auroc > self.stats[\"best_mean_auroc\"]:\n",
        "            print(\"Update best auroc from %f to %f\" % (self.stats['best_mean_auroc'], mean_auroc))\n",
        "\n",
        "            # 1. copy best model\n",
        "            shutil.copy(self.weights_path, self.best_weights_path)\n",
        "\n",
        "            # # # 2. update log file\n",
        "            # print(\"update log file:\", self.best_auroc_log_path)\n",
        "            # with open(self.best_auroc_log_path, \"a\") as f:\n",
        "            #     f.write(\"epoch #%d auroc: %f, lr: %f \\n\" % ((epoch+1), mean_auroc, self.stats['lr']))\n",
        "\n",
        "            # 3. write stats output, this is used for resuming the training\n",
        "            with open(self.stats_output_path, 'w') as f:\n",
        "                json.dump(self.stats, f)\n",
        "\n",
        "            print(\"Update model file: %s -> %s\" % (self.weights_path, self.best_weights_path))\n",
        "            self.stats[\"best_mean_auroc\"] = mean_auroc\n",
        "            print(\"*********************************\")\n",
        "        return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGpATIWFAaY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "output_weights_path = '/content/weights.h5'\n",
        "training_stats = {}\n",
        "generator_workers = 8\n",
        "minimum_lr = 1e-8\n",
        "patience_reduce_lr = 2\n",
        "\n",
        "auroc = MultipleClassAUROC(\n",
        "        sequence=valid_data,\n",
        "        class_names=class_names,\n",
        "        weights_path=output_weights_path,\n",
        "        stats=training_stats,\n",
        "        workers=generator_workers,\n",
        "    )\n",
        "checkpoint = ModelCheckpoint(\n",
        "    output_weights_path,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        ")\n",
        "callbacks = [\n",
        "    checkpoint,\n",
        "    TensorBoard(log_dir=\"/content\", batch_size=batch_size),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
        "                      verbose=1, mode=\"min\", min_lr=minimum_lr),\n",
        "    auroc,\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrVgipRzD4hp",
        "colab_type": "code",
        "outputId": "b147b07b-6b16-41df-92a3-2f0fb82d1cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1737
        }
      },
      "source": [
        "epochs = 5\n",
        "class_weights = 1\n",
        "history = model.fit_generator(\n",
        "        generator=train_data,\n",
        "        steps_per_epoch=None,\n",
        "        epochs=epochs,\n",
        "        validation_data=valid_data,\n",
        "        validation_steps=None,\n",
        "        callbacks=callbacks,\n",
        "        workers=generator_workers,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "with open('out.pkl', 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'history': history.history,\n",
        "            'auroc': auroc.aurocs,\n",
        "        }, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5970/5970 [==============================] - 3425s 574ms/step - loss: 0.4129 - acc: 0.8071 - binary_accuracy: 0.8071 - val_loss: 0.5362 - val_acc: 0.7822 - val_binary_accuracy: 0.7822\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.57778 to 0.53618, saving model to /content/weights.h5\n",
            "\n",
            "*********************************\n",
            "current learning rate: 0.0010000000474974513\n",
            "*** Epoch# 1 dev auroc ***\n",
            "1. Atelectasis: 0.755486\n",
            "2. Cardiomegaly: 0.811052\n",
            "3. Consolidation: 0.899816\n",
            "4. Edema: 0.908482\n",
            "5. Pleural Effusion: 0.915534\n",
            "*********************************\n",
            "mean auroc: 0.858074\n",
            "Update best auroc from 0.000000 to 0.858074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Update model file: /content/weights.h5 -> /content/best_weights.h5\n",
            "*********************************\n",
            "Epoch 2/5\n",
            "5970/5970 [==============================] - 3426s 574ms/step - loss: 0.4112 - acc: 0.8079 - binary_accuracy: 0.8079 - val_loss: 0.4538 - val_acc: 0.7921 - val_binary_accuracy: 0.7921\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.53618 to 0.45381, saving model to /content/weights.h5\n",
            "\n",
            "*********************************\n",
            "current learning rate: 0.0010000000474974513\n",
            "*** Epoch# 2 dev auroc ***\n",
            "1. Atelectasis: 0.821837\n",
            "2. Cardiomegaly: 0.800134\n",
            "3. Consolidation: 0.920956\n",
            "4. Edema: 0.918304\n",
            "5. Pleural Effusion: 0.921875\n",
            "*********************************\n",
            "mean auroc: 0.876621\n",
            "Update best auroc from 0.858074 to 0.876621\n",
            "Update model file: /content/weights.h5 -> /content/best_weights.h5\n",
            "*********************************\n",
            "Epoch 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5970/5970 [==============================] - 3432s 575ms/step - loss: 0.4095 - acc: 0.8084 - binary_accuracy: 0.8084 - val_loss: 0.4611 - val_acc: 0.7842 - val_binary_accuracy: 0.7842\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.45381\n",
            "\n",
            "*********************************\n",
            "current learning rate: 0.0010000000474974513\n",
            "*** Epoch# 3 dev auroc ***\n",
            "1. Atelectasis: 0.744777\n",
            "2. Cardiomegaly: 0.825312\n",
            "3. Consolidation: 0.825184\n",
            "4. Edema: 0.886161\n",
            "5. Pleural Effusion: 0.906929\n",
            "*********************************\n",
            "mean auroc: 0.837673\n",
            "Epoch 4/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5970/5970 [==============================] - 3445s 577ms/step - loss: 0.4084 - acc: 0.8094 - binary_accuracy: 0.8094 - val_loss: 0.4777 - val_acc: 0.7911 - val_binary_accuracy: 0.7911\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.45381\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "*********************************\n",
            "current learning rate: 0.00010000000474974513\n",
            "*** Epoch# 4 dev auroc ***\n",
            "1. Atelectasis: 0.750236\n",
            "2. Cardiomegaly: 0.816065\n",
            "3. Consolidation: 0.932169\n",
            "4. Edema: 0.918452\n",
            "5. Pleural Effusion: 0.908062\n",
            "*********************************\n",
            "mean auroc: 0.864997\n",
            "Epoch 5/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5660/5970 [===========================>..] - ETA: 2:59 - loss: 0.4000 - acc: 0.8134 - binary_accuracy: 0.8134"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}